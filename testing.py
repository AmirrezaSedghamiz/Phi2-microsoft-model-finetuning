import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
from peft import PeftModel
import html
import datetime
import os


def create_persian_test_suite():
    """Comprehensive test suite for Persian language model evaluation"""

    # Test prompts organized by complexity
    test_suite = {
        "basic_language_detection": [
            "Say hello in Persian",
            "What is 'thank you' in Persian?",
            "How do you say 'good morning' in Persian?",
        ],

        "simple_conversation": [
            "Ø³Ù„Ø§Ù…ØŒ Ø­Ø§Ù„Øª Ú†Ø·ÙˆØ±Ù‡ØŸ",
            "Ø§Ù…Ø±ÙˆØ² Ù‡ÙˆØ§ Ú†Ø·ÙˆØ± Ø§Ø³ØªØŸ",
            "Ø§Ø³Ù… ØªÙˆ Ú†ÛŒØ³ØªØŸ",
            "Ú†Ø·ÙˆØ± Ù…ÛŒâ€ŒØªÙˆØ§Ù†Ù… Ø¨Ù‡ ØªÙˆ Ú©Ù…Ú© Ú©Ù†Ù…ØŸ",
        ],

        "translation_requests": [
            "Translate this to Persian: 'I am learning artificial intelligence'",
            "Translate to English: 'Ù…Ù† Ø¯Ø§Ø±Ù… ÙØ§Ø±Ø³ÛŒ ÛŒØ§Ø¯ Ù…ÛŒâ€ŒÚ¯ÛŒØ±Ù…'",
            "How do you say 'machine learning' in Persian?",
        ],

        "cultural_knowledge": [
            "Explain Nowruz (Persian New Year)",
            "Who was Hafez?",
            "What is famous about Persian carpets?",
            "Tell me about Persian poetry",
        ],

        "complex_reasoning": [
            "Write a short story about a cat in Tehran",
            "Explain the importance of education in Persian culture",
            "Describe the process of making Persian tea",
            "What are the main challenges facing Iran today?",
        ],

        "domain_specific": [
            "Explain artificial intelligence in simple Persian",
            "What is quantum computing? Explain in Persian",
            "Describe the internet of things (IoT) in Persian",
        ],

        "creative_writing": [
            "Write a Persian poem about the moon",
            "Create a short dialogue between two friends in Tehran",
            "Write a news headline about technology in Iran",
        ],

        "personality_test": [
            "Tell me a joke in Persian",
            "How are you feeling today?",
            "What's your favorite Persian food?",
            "Can you be my friend?",
        ]
    }
    return test_suite


def load_model_and_tokenizer():
    """Load the fine-tuned model and tokenizer"""
    print("ğŸ”„ Loading model and tokenizer...")

    base_model_path = "G:/model/microsoft--phi-2/models--microsoft--phi-2/snapshots/ef382358ec9e382308935a992d908de099b64c23"
    fine_tuned_path = "G:/phi2-finetuned-large"

    # Load tokenizer
    tokenizer = AutoTokenizer.from_pretrained(
        base_model_path,
        trust_remote_code=True,
        local_files_only=True
    )
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token

    # Try to load fine-tuned model
    try:
        model = AutoModelForCausalLM.from_pretrained(
            fine_tuned_path,
            torch_dtype=torch.float16,
            device_map="cuda",
            trust_remote_code=True,
            local_files_only=True
        )
        print("âœ… Loaded fine-tuned model")
    except Exception as e:
        print(f"âŒ Could not load fine-tuned model: {e}")
        print("ğŸ”„ Loading base model + adapter...")
        try:
            model = AutoModelForCausalLM.from_pretrained(
                base_model_path,
                torch_dtype=torch.float16,
                device_map="cuda",
                trust_remote_code=True,
                local_files_only=True
            )
            model = PeftModel.from_pretrained(model, fine_tuned_path)
            print("âœ… Loaded base model + adapter")
        except Exception as e2:
            print(f"âŒ Failed to load model: {e2}")
            return None, None

    return model, tokenizer


def get_system_prompt():
    """Advanced system prompt engineering to enforce Persian-only responses with friendly personality"""

    system_prompt = """Ø´Ù…Ø§ ÛŒÚ© Ø¯Ø³ØªÛŒØ§Ø± Ù‡ÙˆØ´ Ù…ØµÙ†ÙˆØ¹ÛŒ ÙØ§Ø±Ø³ÛŒâ€ŒØ²Ø¨Ø§Ù† Ù‡Ø³ØªÛŒØ¯ Ú©Ù‡ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ø²ÛŒØ± Ø±Ø§ Ø¯Ø§Ø±ÛŒØ¯:

ğŸ§  **Ù‡ÙˆÛŒØª Ùˆ Ø²Ø¨Ø§Ù†:**
- ÙÙ‚Ø· Ùˆ ÙÙ‚Ø· Ø¨Ù‡ Ø²Ø¨Ø§Ù† ÙØ§Ø±Ø³ÛŒ Ù¾Ø§Ø³Ø® Ù…ÛŒâ€ŒØ¯Ù‡ÛŒØ¯
- Ù‡Ø±Ú¯Ø² Ø§Ø² Ø§Ù†Ú¯Ù„ÛŒØ³ÛŒ ÛŒØ§ Ø²Ø¨Ø§Ù†â€ŒÙ‡Ø§ÛŒ Ø¯ÛŒÚ¯Ø± Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù†Ù…ÛŒâ€ŒÚ©Ù†ÛŒØ¯
- Ø§Ø² Ø²Ø¨Ø§Ù† Ù…Ø­Ø§ÙˆØ±Ù‡â€ŒØ§ÛŒ Ùˆ Ø¯ÙˆØ³ØªØ§Ù†Ù‡ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù…ÛŒâ€ŒÚ©Ù†ÛŒØ¯

â¤ï¸ **Ø´Ø®ØµÛŒØª Ùˆ Ø±ÙØªØ§Ø±:**
- Ø¨Ø³ÛŒØ§Ø± Ø®ÙˆØ´â€ŒØ¨Ø±Ø®ÙˆØ±Ø¯ Ùˆ Ø¯ÙˆØ³ØªØ§Ù†Ù‡ Ù‡Ø³ØªÛŒØ¯
- Ù‡Ù…ÛŒØ´Ù‡ Ù…Ø§ÛŒÙ„ Ø¨Ù‡ Ú©Ù…Ú© Ú©Ø±Ø¯Ù† Ù‡Ø³ØªÛŒØ¯
- Ø¨Ø§ Ú©Ø§Ø±Ø¨Ø±Ø§Ù† Ù…Ø§Ù†Ù†Ø¯ Ø¯ÙˆØ³ØªØ§Ù† Ù‚Ø¯ÛŒÙ…ÛŒ Ø±ÙØªØ§Ø± Ù…ÛŒâ€ŒÚ©Ù†ÛŒØ¯
- Ú©Ù…ÛŒ Ø´ÙˆØ®â€ŒØ·Ø¨Ø¹ Ùˆ ØµÙ…ÛŒÙ…ÛŒ Ù‡Ø³ØªÛŒØ¯

ğŸ¯ **ÙˆØ¸Ø§ÛŒÙ:**
- Ø¨Ù‡ ØªÙ…Ø§Ù… Ø¯Ø±Ø®ÙˆØ§Ø³Øªâ€ŒÙ‡Ø§ÛŒ Ú©Ø§Ø±Ø¨Ø± Ø¨Ù‡ Ø¨Ù‡ØªØ±ÛŒÙ† Ø´Ú©Ù„ Ù¾Ø§Ø³Ø® Ù…ÛŒâ€ŒØ¯Ù‡ÛŒØ¯
- Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ù…ÙÛŒØ¯ Ùˆ Ø¯Ù‚ÛŒÙ‚ Ø§Ø±Ø§Ø¦Ù‡ Ù…ÛŒâ€ŒØ¯Ù‡ÛŒØ¯
- Ø¯Ø± ØµÙˆØ±Øª Ù†ÛŒØ§Ø² ØªÙˆØ¶ÛŒØ­Ø§Øª Ø¨ÛŒØ´ØªØ±ÛŒ Ù…ÛŒâ€ŒØ¯Ù‡ÛŒØ¯
- Ø®Ù„Ø§Ù‚ Ùˆ Ù…Ø¨ØªÚ©Ø± Ù‡Ø³ØªÛŒØ¯

ğŸ“ **Ø³Ø¨Ú© Ù†ÙˆØ´ØªØ§Ø±:**
- Ø§Ø² Ø§ØµØ·Ù„Ø§Ø­Ø§Øª Ùˆ Ø¶Ø±Ø¨â€ŒØ§Ù„Ù…Ø«Ù„â€ŒÙ‡Ø§ÛŒ ÙØ§Ø±Ø³ÛŒ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù…ÛŒâ€ŒÚ©Ù†ÛŒØ¯
- Ù¾Ø§Ø³Ø®â€ŒÙ‡Ø§ Ø±Ø§ Ú¯Ø±Ù… Ùˆ Ø¬Ø°Ø§Ø¨ Ù…ÛŒâ€ŒÚ©Ù†ÛŒØ¯
- Ø§Ø² Ø§Ù…ÙˆØ¬ÛŒ Ù…Ù†Ø§Ø³Ø¨ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù…ÛŒâ€ŒÚ©Ù†ÛŒØ¯ (ÙˆÙ„ÛŒ Ù†Ù‡ Ø²ÛŒØ§Ø¯)
- Ù¾Ø§Ø±Ø§Ú¯Ø±Ø§Ùâ€ŒØ¨Ù†Ø¯ÛŒ Ù…Ù†Ø§Ø³Ø¨ÛŒ Ø¯Ø§Ø±ÛŒØ¯

Ù„Ø·ÙØ§Ù‹ ØªÙ…Ø§Ù… Ù¾Ø§Ø³Ø®â€ŒÙ‡Ø§ÛŒ Ø®ÙˆØ¯ Ø±Ø§ Ø¨Ø± Ø§Ø³Ø§Ø³ Ø§ÛŒÙ† ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ Ø§Ø±Ø§Ø¦Ù‡ Ø¯Ù‡ÛŒØ¯."""

    return system_prompt


def generate_response(model, tokenizer, prompt, max_length=400, temperature=0.8):
    """Generate response with advanced prompt engineering and personality"""

    system_prompt = get_system_prompt()

    # Advanced prompt engineering templates with personality
    prompt_templates = {
        "persian_personality": f"""### Ø³ÛŒØ³ØªÙ…:
{system_prompt}

### Ú©Ø§Ø±Ø¨Ø±:
{prompt}

### Ø¯Ø³ØªÛŒØ§Ø± ÙØ§Ø±Ø³ÛŒ:
""",

        "friendly_chat": f"""ğŸ‘¤ Ú©Ø§Ø±Ø¨Ø±: {prompt}

ğŸ¤– Ø¯Ø³ØªÛŒØ§Ø± ÙØ§Ø±Ø³ÛŒ (Ø¯ÙˆØ³ØªØ§Ù†Ù‡ Ùˆ Ø®ÙˆØ´â€ŒØ¨Ø±Ø®ÙˆØ±Ø¯):""",

        "creative_mode": f"""ğŸ­ **Ø­Ø§Ù„Øª Ø®Ù„Ø§Ù‚Ø§Ù†Ù‡ ÙØ¹Ø§Ù„ Ø´Ø¯**
ğŸ“‹ Ø¯Ø³ØªÙˆØ± Ø³ÛŒØ³ØªÙ…: {system_prompt}

ğŸ’¬ Ø¯Ø±Ø®ÙˆØ§Ø³Øª Ú©Ø§Ø±Ø¨Ø±: Â«{prompt}Â»

âœ¨ Ù¾Ø§Ø³Ø® Ù…Ù† (Ø¨Ù‡ ÙØ§Ø±Ø³ÛŒ Ø¯ÙˆØ³ØªØ§Ù†Ù‡):""",

        "instruction_following": f"""<|system|>
{system_prompt}
<|user|>
{prompt}
<|assistant|>"""
    }

    # Try different templates and return the best one
    best_response = ""
    for template_name, formatted_prompt in prompt_templates.items():
        try:
            inputs = tokenizer(formatted_prompt, return_tensors="pt").to(model.device)

            with torch.no_grad():
                outputs = model.generate(
                    **inputs,
                    max_length=max_length,
                    temperature=temperature,
                    do_sample=True,
                    top_p=0.92,
                    top_k=50,
                    repetition_penalty=1.15,
                    no_repeat_ngram_size=3,
                    pad_token_id=tokenizer.eos_token_id,
                    eos_token_id=tokenizer.eos_token_id,
                    early_stopping=True
                )

            response = tokenizer.decode(outputs[0], skip_special_tokens=True)
            # Extract only the new generated part
            response = response.replace(formatted_prompt, "").strip()

            # Quality checks for the response
            if (len(response) > 10 and
                    any(char in response for char in 'Ø§Ø¨Ù¾ØªØ«Ø¬Ú†Ø­Ø®Ø¯Ø°Ø±Ø²Ú˜Ø³Ø´ØµØ¶Ø·Ø¸Ø¹ØºÙÙ‚Ú©Ú¯Ù„Ù…Ù†ÙˆÙ‡ÛŒ') and
                    len(response) > len(best_response)):
                best_response = response

        except Exception as e:
            continue

    # Fallback if no good response
    if not best_response:
        fallback_responses = [
            "Ø³Ù„Ø§Ù…! Ù…ØªØ£Ø³ÙØ§Ù†Ù‡ Ø¯Ø± Ø­Ø§Ù„ Ø­Ø§Ø¶Ø± Ù†Ù…ÛŒâ€ŒØªÙˆÙ†Ù… Ø¨Ù‡ Ø¯Ø±Ø®ÙˆØ§Ø³Øª Ø´Ù…Ø§ Ù¾Ø§Ø³Ø® Ø¨Ø¯Ù…. Ù„Ø·ÙØ§Ù‹ Ú©Ù…ÛŒ Ø¨Ø¹Ø¯ ØªÙ„Ø§Ø´ Ú©Ù†ÛŒØ¯! ğŸ˜Š",
            "Ø§ÙˆÙ‡! Ø¨Ù‡ Ù†Ø¸Ø± Ù…ÛŒâ€ŒØ±Ø³Ù‡ Ù…Ø´Ú©Ù„ÛŒ Ù¾ÛŒØ´ Ø§ÙˆÙ…Ø¯Ù‡. Ù…ÛŒâ€ŒØªÙˆÙ†ÛŒØ¯ Ø³ÙˆØ§Ù„ Ø®ÙˆØ¯ØªÙˆÙ† Ø±Ùˆ Ø¯ÙˆØ¨Ø§Ø±Ù‡ Ø¨Ù¾Ø±Ø³ÛŒØ¯ØŸ",
            "Ø¨Ø¨Ø®Ø´ÛŒØ¯ØŒ Ù†ØªÙˆÙ†Ø³ØªÙ… Ù…ØªÙˆØ¬Ù‡ Ø¨Ø´Ù…. Ù„Ø·ÙØ§Ù‹ Ø³ÙˆØ§Ù„ Ø®ÙˆØ¯ØªÙˆÙ† Ø±Ùˆ Ø¨Ù‡ ÙØ§Ø±Ø³ÛŒ ÙˆØ§Ø¶Ø­â€ŒØªØ± Ø¨Ù¾Ø±Ø³ÛŒØ¯! ğŸŒŸ"
        ]
        best_response = fallback_responses[hash(prompt) % len(fallback_responses)]

    return best_response


def create_html_report(test_results, output_path):
    """Create a beautiful HTML report with RTL support for Persian"""

    html_content = f"""
    <!DOCTYPE html>
    <html dir="rtl" lang="fa">
    <head>
        <meta charset="UTF-8">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <title>Persian Language Model Evaluation Report</title>
        <style>
            body {{
                font-family: 'Tahoma', 'Arial', sans-serif;
                line-height: 1.8;
                margin: 0;
                padding: 20px;
                background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
                min-height: 100vh;
            }}
            .container {{
                max-width: 1200px;
                margin: 0 auto;
                background: white;
                padding: 40px;
                border-radius: 15px;
                box-shadow: 0 20px 40px rgba(0,0,0,0.1);
            }}
            .header {{
                text-align: center;
                border-bottom: 4px solid #2c5faa;
                padding-bottom: 25px;
                margin-bottom: 35px;
                background: linear-gradient(135deg, #2c5faa, #3a7bd5);
                color: white;
                padding: 30px;
                margin: -40px -40px 35px -40px;
                border-radius: 15px 15px 0 0;
            }}
            .header h1 {{
                color: white;
                font-size: 2.8em;
                margin: 0;
                text-shadow: 2px 2px 4px rgba(0,0,0,0.3);
            }}
            .header p {{
                font-size: 1.3em;
                opacity: 0.9;
                margin: 10px 0 0 0;
            }}
            .test-category {{
                background: linear-gradient(135deg, #e8f4fd, #d4e7fa);
                padding: 20px;
                margin: 25px 0;
                border-radius: 12px;
                border-right: 6px solid #2c5faa;
                transition: transform 0.3s ease;
            }}
            .test-category:hover {{
                transform: translateY(-5px);
                box-shadow: 0 10px 25px rgba(0,0,0,0.15);
            }}
            .test-category h2 {{
                color: #2c5faa;
                margin-top: 0;
                font-size: 1.6em;
                border-bottom: 2px solid #2c5faa;
                padding-bottom: 10px;
            }}
            .test-item {{
                background: white;
                margin: 20px 0;
                padding: 20px;
                border-radius: 10px;
                border: 2px solid #e1e8ed;
                transition: all 0.3s ease;
            }}
            .test-item:hover {{
                border-color: #2c5faa;
                box-shadow: 0 5px 15px rgba(44, 95, 170, 0.1);
            }}
            .prompt {{
                background: linear-gradient(135deg, #f0f8ff, #e6f3ff);
                padding: 15px;
                border-radius: 8px;
                margin-bottom: 15px;
                font-weight: bold;
                border-right: 4px solid #4CAF50;
                font-size: 1.1em;
            }}
            .response {{
                background: linear-gradient(135deg, #fff9e6, #fff0cc);
                padding: 20px;
                border-radius: 8px;
                border-right: 4px solid #FF9800;
                white-space: pre-wrap;
                font-size: 1.15em;
                line-height: 1.8;
            }}
            .english {{
                direction: ltr;
                text-align: left;
                font-family: 'Courier New', monospace;
                background: linear-gradient(135deg, #f8f9fa, #e9ecef) !important;
            }}
            .persian {{
                font-size: 1.25em;
                line-height: 2;
                font-family: 'Tahoma', 'Arial', sans-serif;
            }}
            .timestamp {{
                text-align: center;
                color: #666;
                margin-top: 40px;
                font-style: italic;
                padding: 20px;
                background: #f8f9fa;
                border-radius: 10px;
            }}
            .quality-badge {{
                display: inline-block;
                background: #4CAF50;
                color: white;
                padding: 8px 16px;
                border-radius: 20px;
                font-size: 0.9em;
                margin: 10px 5px;
                box-shadow: 0 2px 5px rgba(0,0,0,0.2);
            }}
            .personality-note {{
                background: linear-gradient(135deg, #ffeaa7, #fab1a0);
                padding: 15px;
                border-radius: 10px;
                margin: 20px 0;
                border-right: 4px solid #e17055;
                font-weight: bold;
            }}
        </style>
    </head>
    <body>
        <div class="container">
            <div class="header">
                <h1>ğŸ§ª Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ Ù…Ø¯Ù„ Ø²Ø¨Ø§Ù†ÛŒ ÙØ§Ø±Ø³ÛŒ</h1>
                <p>ØªØ³Øª Ø¬Ø§Ù…Ø¹ Ù…Ø¯Ù„ Phi-2 ØªÙ†Ø¸ÛŒÙ…â€ŒØ´Ø¯Ù‡ Ø¨Ø§ Ù‚Ø§Ø¨Ù„ÛŒØªâ€ŒÙ‡Ø§ÛŒ ÙØ§Ø±Ø³ÛŒ Ùˆ Ø´Ø®ØµÛŒØª Ø¯ÙˆØ³ØªØ§Ù†Ù‡</p>
            </div>

            <div class="personality-note">
                ğŸ“ <strong>Ù†Ú©ØªÙ‡:</strong> Ù…Ø¯Ù„ Ø¨Ø§ Ø¯Ø³ØªÙˆØ±Ø§Ù„Ø¹Ù…Ù„ Ø³ÛŒØ³ØªÙ… Ø¢Ù…ÙˆØ²Ø´ Ø¯ÛŒØ¯Ù‡ ØªØ§:
                <span class="quality-badge">ÙÙ‚Ø· ÙØ§Ø±Ø³ÛŒ ØµØ­Ø¨Øª Ú©Ù†Ø¯</span>
                <span class="quality-badge">Ø´Ø®ØµÛŒØª Ø¯ÙˆØ³ØªØ§Ù†Ù‡ Ø¯Ø§Ø´ØªÙ‡ Ø¨Ø§Ø´Ø¯</span>
                <span class="quality-badge">Ù‡Ù…ÛŒØ´Ù‡ Ú©Ù…Ú©â€ŒÚ©Ù†Ù†Ø¯Ù‡ Ø¨Ø§Ø´Ø¯</span>
                <span class="quality-badge">Ø®Ù„Ø§Ù‚ Ùˆ Ø¬Ø°Ø§Ø¨ Ù¾Ø§Ø³Ø® Ø¯Ù‡Ø¯</span>
            </div>
    """

    # Add test results by category
    for category, tests in test_results.items():
        html_content += f"""
            <div class="test-category">
                <h2>ğŸ“‚ {category.replace('_', ' ').title()}</h2>
        """

        for i, (prompt, response) in enumerate(tests):
            prompt_class = "english" if any(char.isascii() and char.isalpha() for char in prompt) else "persian"
            response_class = "persian"  # Force Persian for responses

            html_content += f"""
                <div class="test-item">
                    <div class="prompt {prompt_class}">ğŸ—¨ï¸ <strong>Ù¾Ø±Ø³Ø´:</strong> {html.escape(prompt)}</div>
                    <div class="response {response_class}">ğŸ¤– <strong>Ù¾Ø§Ø³Ø®:</strong>\n{html.escape(response)}</div>
                </div>
            """

        html_content += "</div>"

    # Add footer
    html_content += f"""
            <div class="timestamp">
                ğŸ•’ ØªÙˆÙ„ÛŒØ¯ Ø´Ø¯Ù‡ Ø¯Ø±: {datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S")}
                <br>
                ğŸ¯ Ù…Ø¯Ù„: Phi-2 ØªÙ†Ø¸ÛŒÙ…â€ŒØ´Ø¯Ù‡ Ø¨Ø§ QLoRA Ø±ÙˆÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ ÙØ§Ø±Ø³ÛŒ
            </div>
        </div>
    </body>
    </html>
    """

    with open(output_path, 'w', encoding='utf-8') as f:
        f.write(html_content)

    print(f"âœ… HTML report saved to: {output_path}")


def analyze_personality_metrics(responses):
    """Analyze how well the model follows personality instructions"""
    metrics = {
        'persian_only': 0,
        'friendly_tone': 0,
        'helpful_content': 0,
        'creative_elements': 0,
        'emoji_usage': 0
    }

    for response in responses:
        # Check for Persian characters
        if any(char in response for char in 'Ø§Ø¨Ù¾ØªØ«Ø¬Ú†Ø­Ø®Ø¯Ø°Ø±Ø²Ú˜Ø³Ø´ØµØ¶Ø·Ø¸Ø¹ØºÙÙ‚Ú©Ú¯Ù„Ù…Ù†ÙˆÙ‡ÛŒ'):
            metrics['persian_only'] += 1

        # Check for friendly words
        friendly_words = ['Ø³Ù„Ø§Ù…', 'Ù„Ø·ÙØ§', 'Ø®ÙˆØ´Ø­Ø§Ù„', 'Ú©Ù…Ú©', 'Ø¯ÙˆØ³Øª', 'Ø®ÙˆØ´', 'Ù…Ù…Ù†ÙˆÙ†', 'Ø¹Ø§Ù„ÛŒ']
        if any(word in response for word in friendly_words):
            metrics['friendly_tone'] += 1

        # Check for helpful content (length and substance)
        if len(response.split()) > 10:
            metrics['helpful_content'] += 1

        # Check for creative elements
        creative_indicators = ['!', 'ØŸ', '...', 'âœ¨', 'ğŸ˜Š', 'ğŸŒŸ', 'ğŸ¯']
        if any(indicator in response for indicator in creative_indicators):
            metrics['creative_elements'] += 1

        # Check emoji usage
        if any(char in response for char in ['ğŸ˜Š', 'ğŸ˜‚', 'â¤ï¸', 'âœ¨', 'ğŸŒŸ', 'ğŸ¯', 'ğŸ§ ']):
            metrics['emoji_usage'] += 1

    # Convert to percentages
    total_responses = len(responses)
    for key in metrics:
        metrics[key] = (metrics[key] / total_responses) * 100

    return metrics


def main():
    """Main testing function"""
    print("ğŸš€ Starting Advanced Persian Language Model Evaluation")
    print("ğŸ­ System Prompt: Persian-only, friendly personality enforced")

    # Load model
    model, tokenizer = load_model_and_tokenizer()
    if model is None:
        print("âŒ Failed to load model. Exiting.")
        return

    # Create test suite
    test_suite = create_persian_test_suite()
    test_results = {}
    all_responses = []

    print("ğŸ§ª Running comprehensive personality-enforced tests...")
    print("ğŸ“‹ System instructions: Persian-only, friendly, helpful, creative")

    # Run all tests
    for category, prompts in test_suite.items():
        print(f"ğŸ“‹ Testing: {category}")
        category_results = []

        for prompt in prompts:
            print(f"   Testing: {prompt[:50]}...")
            response = generate_response(model, tokenizer, prompt)
            category_results.append((prompt, response))
            all_responses.append(response)

        test_results[category] = category_results

    # Analyze personality metrics
    print("\nğŸ“Š Analyzing personality adherence...")
    metrics = analyze_personality_metrics(all_responses)

    print("ğŸ¯ Personality Metrics:")
    for metric, score in metrics.items():
        print(f"   {metric}: {score:.1f}%")

    # Generate HTML report
    output_path = "persian_personality_evaluation_report.html"
    create_html_report(test_results, output_path)

    # Generate metrics summary
    with open("personality_metrics.txt", "w", encoding="utf-8") as f:
        f.write("Ø´Ø§Ø®Øµâ€ŒÙ‡Ø§ÛŒ Ø´Ø®ØµÛŒØª Ù…Ø¯Ù„ ÙØ§Ø±Ø³ÛŒ\n")
        f.write("=" * 50 + "\n\n")
        for metric, score in metrics.items():
            f.write(f"{metric}: {score:.1f}%\n")

    print("âœ… Evaluation complete!")
    print(f"ğŸ“Š HTML Report: {output_path}")
    print(f"ğŸ“ˆ Metrics: personality_metrics.txt")

    # Print quick summary to console
    print("\n" + "=" * 70)
    print("QUICK PERSONALITY SUMMARY (Check HTML for proper Persian rendering):")
    print("=" * 70)

    for category, tests in test_results.items():
        print(f"\nğŸ“‚ {category.upper()}:")
        for prompt, response in tests[:2]:
            print(f"   Prompt: {prompt[:60]}...")
            print(f"   Response: {response[:100]}...")
            print()


if __name__ == "__main__":
    main()