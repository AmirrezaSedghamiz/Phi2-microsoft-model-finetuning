ğŸ§  QLoRA Fine-tuning of Phi-2 for Persian Language Understanding
https://img.shields.io/badge/Python-3.9+-blue.svg
https://img.shields.io/badge/PyTorch-2.0+-red.svg
https://img.shields.io/badge/HuggingFace-Transformers-yellow.svg
https://img.shields.io/badge/Training-QLoRA-green.svg
https://img.shields.io/badge/License-MIT-green.svg
https://img.shields.io/badge/Dataset-7M%252B_Persian-orange.svg

Advanced QLoRA fine-tuning of Microsoft's Phi-2 model on large Persian datasets with resource-efficient training, comprehensive evaluation, and personality-enforced Persian-only responses.

ğŸ“‹ Project Overview
This project implements efficient fine-tuning of Phi-2 (2.7B parameters) using QLoRA (Quantized Low-Rank Adaptation) on 7M+ Persian samples. The model is trained to respond exclusively in Persian with a friendly, helpful personality while maintaining resource efficiency (50% GPU/CPU limits).

ğŸ¯ Key Features
âœ… Resource-Efficient Training: Dynamic resource monitoring with enforced limits (50% GPU, 50% CPU, 33% RAM)

âœ… Streaming Data Pipeline: Handles 7M+ samples without memory overload

âœ… Graceful Recovery: Automatic checkpointing and resume functionality

âœ… Personality Engineering: Enforces Persian-only responses with friendly AI persona

âœ… Comprehensive Evaluation: 8 test categories with HTML reporting

âœ… Production Ready: Robust error handling and emergency saves

ğŸ—ï¸ Project Structure
text
QLoRa-FineTuning/
â”œâ”€â”€ ğŸ“ Data/                          # Dataset directory (not in Git - large files)
â”‚   â”œâ”€â”€ General Data/                # Persian news, blogs, Q&A datasets
â”‚   â””â”€â”€ Advising Data/               # Specialized Persian datasets
â”‚
â”œâ”€â”€ ğŸ“ src/                          # Source code
â”‚   â”œâ”€â”€ main.py                     # ğŸš€ Main training script (optimized)
â”‚   â”œâ”€â”€ main_optimized.py           # Original optimized version
â”‚   â”œâ”€â”€ test_model.py               # ğŸ§ª Interactive model testing
â”‚   â”œâ”€â”€ testing.py                  # ğŸ“Š Comprehensive evaluation suite
â”‚   â””â”€â”€ exam.py                     # ğŸ” Graph algorithm implementation
â”‚
â”œâ”€â”€ ğŸ“ outputs/                      # Training outputs (not in Git)
â”‚   â”œâ”€â”€ checkpoints/                # Model checkpoints
â”‚   â”œâ”€â”€ logs/                       # Training logs
â”‚   â””â”€â”€ evaluation_results/         # Evaluation outputs
â”‚
â”œâ”€â”€ ğŸ“„ requirements.txt             # Python dependencies
â”œâ”€â”€ ğŸ“„ training.log                 # Training progress log
â”œâ”€â”€ ğŸ“„ README.md                    # This file
â””â”€â”€ ğŸ“„ .gitignore                   # Git ignore rules
ğŸš€ Quick Start
1. Installation
bash
# Clone repository
git clone https://github.com/yourusername/QLoRa-FineTuning.git
cd QLoRa-FineTuning

# Install dependencies
pip install -r requirements.txt

# Additional RTL support for Persian display
pip install arabic-reshaper python-bidi
2. Download Base Model
bash
# Download Phi-2 from HuggingFace
python -c "from transformers import AutoModel; AutoModel.from_pretrained('microsoft/phi-2', cache_dir='G:/model')"
3. Run Training (7M samples)
bash
python src/main.py \
  --dataset "G:/persian_news_processed.jsonl" \
  --local_model_path "G:/model/microsoft--phi-2/models--microsoft--phi-2/snapshots/ef382358ec9e382308935a992d908de099b64c23" \
  --output_dir "./phi2-finetuned" \
  --max_seq_length 512 \
  --per_device_train_batch_size 1 \
  --gradient_accumulation_steps 16 \
  --num_train_epochs 1 \
  --learning_rate 2e-4 \
  --lora_r 16 \
  --lora_alpha 32 \
  --max_samples 7000000 \
  --save_steps 500 \
  --logging_steps 50
4. Test the Model
bash
# Interactive testing
python src/test_model.py --model_path "./phi2-finetuned"

# Comprehensive evaluation with HTML report
python src/testing.py
ğŸ”§ Technical Implementation
ğŸ—ï¸ Architecture
python
Base Model: Microsoft Phi-2 (2.7B parameters)
Fine-tuning: QLoRA (4-bit quantization + Low-Rank Adaptation)
Adapter Rank: r=16, alpha=32
Training Precision: bfloat16/fp16
Sequence Length: 512 tokens
ğŸ“Š Resource Management
GPU Utilization: Max 50% with dynamic throttling

CPU Utilization: Max 50% with task scheduling

RAM Management: Max 33% with garbage collection

Streaming Pipeline: Processes 7M+ samples without loading all into memory

Checkpointing: Automatic saves every 500 steps

ğŸ­ Personality Engineering
The model is trained with strict system prompts enforcing:

Persian-only responses - No English code-switching

Friendly persona - Warm, helpful AI assistant

Cultural awareness - Understanding of Persian culture

Creative responses - Engaging and thoughtful answers

ğŸ“ˆ Evaluation Results
ğŸ§ª Test Categories
Category	Samples	Purpose
Basic Language	3	Persian language detection
Conversation	4	Natural Persian dialogue
Translation	3	Bi-directional translation
Cultural Knowledge	4	Persian culture understanding
Complex Reasoning	4	Advanced Persian reasoning
Domain Specific	3	Technical Persian explanations
Creative Writing	3	Persian creative content
Personality	4	Friendly AI behavior
ğŸ“Š Performance Metrics
yaml
Persian Language Adherence: 100%
Helpful Content Generation: 100%
Friendly Tone: 25%
Creative Elements: 28.6%
Emoji Usage (Appropriate): 7.1%
ğŸ’¾ Dataset Information
ğŸ“š Composition
Dataset	Samples	Size	Description
Persian News	~5M	~30GB	Modern Persian news articles
Persian Blogs	~1M	~8GB	Informal Persian blog posts
Persian Q&A	~1M	~6GB	Question-answer pairs
Total	~7M	~44GB	Comprehensive Persian corpus
ğŸ”„ Data Processing Pipeline
python
1. Streaming JSONL Loading â†’ 2. Persian Text Extraction â†’ 
3. Quality Filtering â†’ 4. Tokenization â†’ 
5. Sequence Padding â†’ 6. Batch Generation
ğŸ› ï¸ Advanced Usage
Resume Training from Checkpoint
bash
python src/main.py \
  --resume_from_checkpoint "./phi2-finetuned/checkpoint-5000" \
  # ... other parameters
Custom Evaluation
python
from testing import create_persian_test_suite, generate_response

# Create custom test suite
tests = {
    "my_category": ["Ø³ÙˆØ§Ù„ ÙØ§Ø±Ø³ÛŒ Û±", "Ø³ÙˆØ§Ù„ ÙØ§Ø±Ø³ÛŒ Û²"]
}

# Run evaluation
results = {}
for category, prompts in tests.items():
    responses = [generate_response(model, tokenizer, p) for p in prompts]
    results[category] = list(zip(prompts, responses))
Export for Production
python
# Convert to single model
model.save_pretrained("./production_model", safe_serialization=True)
tokenizer.save_pretrained("./production_model")
ğŸ“ Key Files Explained
File	Purpose	Key Features
main.py	ğŸš€ Main training	Streaming data, QLoRA, resource limits
test_model.py	ğŸ§ª Interactive testing	RTL Persian display, streaming responses
testing.py	ğŸ“Š Evaluation suite	8 test categories, HTML reports, personality metrics
exam.py	ğŸ” Algorithm	Graph cycle detection, component analysis
ğŸ¯ Technical Challenges Solved
Memory Management: Streaming pipeline for 7M+ samples

Resource Efficiency: Dynamic GPU/CPU/RAM limiting

Persian RTL Support: Proper Arabic script rendering

Personality Enforcement: System prompt engineering for Persian-only responses

Training Stability: Gradient checkpointing, mixed precision, LoRA stabilization

ğŸ“Š Model Specifications
Parameter	Value	Description
Base Model	Phi-2 (2.7B)	Microsoft's compact LM
Fine-tuning	QLoRA	4-bit quantized LoRA
Trainable Params	~4.1M	0.15% of total parameters
Sequence Length	512	Optimized for Persian
Batch Size	1 Ã— 16	Effective batch size 16
Learning Rate	2e-4	Cosine scheduler
Training Time	~72 hours	On single RTX 3090
ğŸ” Evaluation Examples
âœ… Successful Responses
persian
ğŸ¯ **Ù¾Ø±Ø³Ø´**: Ø³Ù„Ø§Ù…ØŒ Ø­Ø§Ù„Øª Ú†Ø·ÙˆØ±Ù‡ØŸ
ğŸ¤– **Ù¾Ø§Ø³Ø®**: Ø³Ù„Ø§Ù… Ø¹Ø²ÛŒØ²Ù…! ğŸ˜Š Ø®Ø¯Ø§ Ø±Ùˆ Ø´Ú©Ø± Ø®ÛŒÙ„ÛŒ Ø®ÙˆØ¨Ù…ØŒ Ø§Ù…ÛŒØ¯ÙˆØ§Ø±Ù… ØªÙˆ Ù‡Ù… Ø­Ø§Ù„Øª Ø¹Ø§Ù„ÛŒ Ø¨Ø§Ø´Ù‡. Ú†Ù‡ Ø®Ø¨Ø±Ø§ØŸ Ú†Ø·ÙˆØ± Ù…ÛŒâ€ŒØªÙˆÙ†Ù… Ú©Ù…Ú©Øª Ú©Ù†Ù… Ø§Ù…Ø±ÙˆØ²ØŸ

ğŸ¯ **Ù¾Ø±Ø³Ø´**: Explain artificial intelligence in simple Persian
ğŸ¤– **Ù¾Ø§Ø³Ø®**: Ù‡ÙˆØ´ Ù…ØµÙ†ÙˆØ¹ÛŒ Ù…Ø«Ù„ ÛŒÚ© Ø¨Ú†Ù‡ Ø¨Ø§Ù‡ÙˆØ´Ù‡ Ú©Ù‡ Ù…ÛŒâ€ŒØªÙˆÙ†Ù‡ Ú†ÛŒØ²Ø§ÛŒ Ø¬Ø¯ÛŒØ¯ ÛŒØ§Ø¯ Ø¨Ú¯ÛŒØ±Ù‡! ğŸ§  Ø§ÛŒÙ† ÙÙ†Ø§ÙˆØ±ÛŒ Ø¨Ù‡ Ú©Ø§Ù…Ù¾ÛŒÙˆØªØ±Ù‡Ø§ Ú©Ù…Ú© Ù…ÛŒâ€ŒÚ©Ù†Ù‡ ØªØ§ ÙÚ©Ø± Ú©Ù†Ù†Ø¯ØŒ ÛŒØ§Ø¯ Ø¨Ú¯ÛŒØ±Ù†Ø¯ Ùˆ ØªØµÙ…ÛŒÙ… Ø¨Ú¯ÛŒØ±Ù†Ø¯...
ğŸ“ˆ Personality Metrics
100% Persian adherence - No English leakage

Natural conversation flow - Contextually appropriate

Cultural relevance - Persian idioms and references

Friendly tone - Warm and approachable

ğŸš¨ Troubleshooting
Common Issues
bash
# Out of memory
Solution: Reduce batch size, enable gradient checkpointing

# Slow training
Solution: Increase gradient accumulation steps, optimize data loading

# Poor Persian quality
Solution: Adjust temperature (0.7-0.9), review training data quality

# Checkpoint loading failed
Solution: Ensure same model architecture, check file permissions
Performance Tips
yaml
For faster training:
  - Use flash attention if available
  - Increase gradient accumulation steps
  - Enable mixed precision (bfloat16)
  - Use smaller LoRA rank (r=8)

For better quality:
  - Increase training epochs (2-3)
  - Use larger LoRA rank (r=32)
  - Adjust learning rate (1e-4 to 3e-4)
  - Add more diverse Persian data
ğŸ“š References & Citations
Academic Papers
QLoRA: Efficient Fine-tuning of Quantized LLMs

Phi-2: The Surprising Reasoning Power of Small LMs

Low-Rank Adaptation (LoRA) for LLMs

Libraries Used
HuggingFace Transformers

PEFT (Parameter-Efficient Fine-Tuning)

BitsAndBytes

Persian NLP Resources
Persian NLP Datasets

ğŸ“„ License
This project is licensed under the MIT License - see the LICENSE file for details.

ğŸ™ Acknowledgments
Microsoft Research for the Phi-2 base model

HuggingFace for Transformers and PEFT libraries

Persian NLP Community for datasets and resources

QLoRA authors for efficient fine-tuning method

ğŸ“§ Contact
Your Name - GitHub Profile

Project Link: https://github.com/yourusername/QLoRa-FineTuning

â­ Show Your Support
If you find this project useful, please give it a star! â­

ğŸ¯ Skills Demonstrated
This project showcases expertise in:

Large Language Model fine-tuning with QLoRA

Resource-efficient training on limited hardware

Persian NLP and RTL text processing

Comprehensive evaluation with personality metrics

Production-ready ML pipelines with error handling

Streaming data processing for large datasets

Built with â¤ï¸ for the Persian AI community
HuggingFace Persian Models

