import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
from peft import PeftModel
import html
import datetime
import os

def create_persian_test_suite():
    """Comprehensive test suite for Persian language model evaluation"""

    # Test prompts organized by complexity
    test_suite = {
        "basic_language_detection": [
            "Say hello in Persian",
            "What is 'thank you' in Persian?",
            "How do you say 'good morning' in Persian?",
        ],

        "simple_conversation": [
            "ÿ≥ŸÑÿßŸÖÿå ÿ≠ÿßŸÑÿ™ ⁄Üÿ∑Ÿàÿ±Ÿáÿü",
            "ÿßŸÖÿ±Ÿàÿ≤ ŸáŸàÿß ⁄Üÿ∑Ÿàÿ± ÿßÿ≥ÿ™ÿü",
            "ÿßÿ≥ŸÖ ÿ™Ÿà ⁄Ü€åÿ≥ÿ™ÿü",
            "⁄Üÿ∑Ÿàÿ± ŸÖ€å‚Äåÿ™ŸàÿßŸÜŸÖ ÿ®Ÿá ÿ™Ÿà ⁄©ŸÖ⁄© ⁄©ŸÜŸÖÿü",
        ],

        "translation_requests": [
            "Translate this to Persian: 'I am learning artificial intelligence'",
            "Translate to English: 'ŸÖŸÜ ÿØÿßÿ±ŸÖ ŸÅÿßÿ±ÿ≥€å €åÿßÿØ ŸÖ€å‚Äå⁄Ø€åÿ±ŸÖ'",
            "How do you say 'machine learning' in Persian?",
        ],

        "cultural_knowledge": [
            "Explain Nowruz (Persian New Year)",
            "Who was Hafez?",
            "What is famous about Persian carpets?",
            "Tell me about Persian poetry",
        ],

        "complex_reasoning": [
            "Write a short story about a cat in Tehran",
            "Explain the importance of education in Persian culture",
            "Describe the process of making Persian tea",
            "What are the main challenges facing Iran today?",
        ],

        "domain_specific": [
            "Explain artificial intelligence in simple Persian",
            "What is quantum computing? Explain in Persian",
            "Describe the internet of things (IoT) in Persian",
        ],

        "creative_writing": [
            "Write a Persian poem about the moon",
            "Create a short dialogue between two friends in Tehran",
            "Write a news headline about technology in Iran",
        ]
    }
    return test_suite

def load_model_and_tokenizer():
    """Load the fine-tuned model and tokenizer"""
    print("üîÑ Loading model and tokenizer...")

    base_model_path = "G:/model/microsoft--phi-2/models--microsoft--phi-2/snapshots/ef382358ec9e382308935a992d908de099b64c23"
    fine_tuned_path = "G:/phi2-finetuned-large"

    # Load tokenizer
    tokenizer = AutoTokenizer.from_pretrained(
        base_model_path,
        trust_remote_code=True,
        local_files_only=True
    )
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token

    # Try to load fine-tuned model
    try:
        model = AutoModelForCausalLM.from_pretrained(
            fine_tuned_path,
            torch_dtype=torch.float16,
            device_map="auto",
            trust_remote_code=True,
            local_files_only=True
        )
        print("‚úÖ Loaded fine-tuned model")
    except Exception as e:
        print(f"‚ùå Could not load fine-tuned model: {e}")
        print("üîÑ Loading base model + adapter...")
        try:
            model = AutoModelForCausalLM.from_pretrained(
                base_model_path,
                torch_dtype=torch.float16,
                device_map="auto",
                trust_remote_code=True,
                local_files_only=True
            )
            model = PeftModel.from_pretrained(model, fine_tuned_path)
            print("‚úÖ Loaded base model + adapter")
        except Exception as e2:
            print(f"‚ùå Failed to load model: {e2}")
            return None, None

    return model, tokenizer

def generate_response(model, tokenizer, prompt, max_length=300, temperature=0.7):
    """Generate response with proper prompt engineering"""

    # Enhanced prompt engineering templates
    prompt_templates = {
        "instruction": f"### Instruction:\n{prompt}\n\n### Response:\n",
        "conversational": f"User: {prompt}\nAssistant:",
        "qa": f"Question: {prompt}\nAnswer:",
        "creative": f"### Task: {prompt}\n\n### Output:\n"
    }

    # Try different templates and return the best one
    best_response = ""
    for template_name, formatted_prompt in prompt_templates.items():
        try:
            inputs = tokenizer(formatted_prompt, return_tensors="pt").to(model.device)

            with torch.no_grad():
                outputs = model.generate(
                    **inputs,
                    max_length=max_length,
                    temperature=temperature,
                    do_sample=True,
                    top_p=0.9,
                    repetition_penalty=1.1,
                    pad_token_id=tokenizer.eos_token_id,
                    eos_token_id=tokenizer.eos_token_id,
                )

            response = tokenizer.decode(outputs[0], skip_special_tokens=True)
            # Extract only the new generated part
            response = response.replace(formatted_prompt, "").strip()

            if len(response) > len(best_response):
                best_response = response

        except Exception as e:
            continue

    return best_response if best_response else "No response generated"

def create_html_report(test_results, output_path):
    """Create a beautiful HTML report with RTL support for Persian"""

    html_content = f"""
    <!DOCTYPE html>
    <html dir="rtl" lang="fa">
    <head>
        <meta charset="UTF-8">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <title>Persian Language Model Evaluation Report</title>
        <style>
            body {{
                font-family: 'Tahoma', 'Arial', sans-serif;
                line-height: 1.8;
                margin: 0;
                padding: 20px;
                background-color: #f5f5f5;
            }}
            .container {{
                max-width: 1200px;
                margin: 0 auto;
                background: white;
                padding: 30px;
                border-radius: 10px;
                box-shadow: 0 0 20px rgba(0,0,0,0.1);
            }}
            .header {{
                text-align: center;
                border-bottom: 3px solid #2c5faa;
                padding-bottom: 20px;
                margin-bottom: 30px;
            }}
            .header h1 {{
                color: #2c5faa;
                font-size: 2.5em;
                margin: 0;
            }}
            .test-category {{
                background: #e8f4fd;
                padding: 15px;
                margin: 20px 0;
                border-radius: 8px;
                border-right: 5px solid #2c5faa;
            }}
            .test-category h2 {{
                color: #2c5faa;
                margin-top: 0;
            }}
            .test-item {{
                background: white;
                margin: 15px 0;
                padding: 15px;
                border-radius: 5px;
                border: 1px solid #ddd;
            }}
            .prompt {{
                background: #f0f8ff;
                padding: 10px;
                border-radius: 5px;
                margin-bottom: 10px;
                font-weight: bold;
                border-right: 3px solid #4CAF50;
            }}
            .response {{
                background: #fff9e6;
                padding: 10px;
                border-radius: 5px;
                border-right: 3px solid #FF9800;
                white-space: pre-wrap;
                font-size: 1.1em;
            }}
            .english {{
                direction: ltr;
                text-align: left;
                font-family: 'Courier New', monospace;
            }}
            .persian {{
                font-size: 1.2em;
                line-height: 2;
            }}
            .timestamp {{
                text-align: center;
                color: #666;
                margin-top: 30px;
                font-style: italic;
            }}
            .rating {{
                background: #4CAF50;
                color: white;
                padding: 5px 10px;
                border-radius: 15px;
                font-size: 0.9em;
                margin-left: 10px;
            }}
        </style>
    </head>
    <body>
        <div class="container">
            <div class="header">
                <h1>üß™ Persian Language Model Evaluation Report</h1>
                <p>Comprehensive testing of fine-tuned Phi-2 model on Persian language tasks</p>
            </div>
    """

    # Add test results by category
    for category, tests in test_results.items():
        html_content += f"""
            <div class="test-category">
                <h2>üìÇ {category.replace('_', ' ').title()}</h2>
        """

        for i, (prompt, response) in enumerate(tests):
            prompt_class = "english" if any(char.isascii() and char.isalpha() for char in prompt) else "persian"
            response_class = "english" if any(char.isascii() and char.isalpha() for char in response) else "persian"

            html_content += f"""
                <div class="test-item">
                    <div class="prompt {prompt_class}">üó®Ô∏è <strong>Prompt:</strong> {html.escape(prompt)}</div>
                    <div class="response {response_class}">ü§ñ <strong>Response:</strong>\n{html.escape(response)}</div>
                </div>
            """

        html_content += "</div>"

    # Add footer
    html_content += f"""
            <div class="timestamp">
                Generated on: {datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S")}
            </div>
        </div>
    </body>
    </html>
    """

    with open(output_path, 'w', encoding='utf-8') as f:
        f.write(html_content)

    print(f"‚úÖ HTML report saved to: {output_path}")

def main():
    """Main testing function"""
    print("üöÄ Starting Persian Language Model Evaluation")

    # Load model
    model, tokenizer = load_model_and_tokenizer()
    if model is None:
        print("‚ùå Failed to load model. Exiting.")
        return

    # Create test suite
    test_suite = create_persian_test_suite()
    test_results = {}

    print("üß™ Running comprehensive tests...")

    # Run all tests
    for category, prompts in test_suite.items():
        print(f"üìã Testing: {category}")
        category_results = []

        for prompt in prompts:
            print(f"   Testing: {prompt[:50]}...")
            response = generate_response(model, tokenizer, prompt)
            category_results.append((prompt, response))

        test_results[category] = category_results

    # Generate HTML report
    output_path = "persian_model_evaluation_report.html"
    create_html_report(test_results, output_path)

    # Also create a simple text summary
    with open("persian_model_summary.txt", "w", encoding="utf-8") as f:
        f.write("Persian Language Model Evaluation Summary\n")
        f.write("=" * 50 + "\n\n")

        for category, tests in test_results.items():
            f.write(f"CATEGORY: {category.upper()}\n")
            f.write("-" * 30 + "\n")

            for prompt, response in tests:
                f.write(f"PROMPT: {prompt}\n")
                f.write(f"RESPONSE: {response}\n")
                f.write("-" * 20 + "\n")
            f.write("\n")

    print("‚úÖ Evaluation complete!")
    print(f"üìä HTML Report: {output_path}")
    print(f"üìù Text Summary: persian_model_summary.txt")

    # Print quick summary to console
    print("\n" + "="*60)
    print("QUICK SUMMARY (Check HTML file for proper Persian rendering):")
    print("="*60)

    for category, tests in test_results.items():
        print(f"\nüìÇ {category.upper()}:")
        for prompt, response in tests[:2]:  # Show first 2 from each category
            print(f"   Prompt: {prompt[:60]}...")
            print(f"   Response: {response[:80]}...")
            print()

if __name__ == "__main__":
    main()